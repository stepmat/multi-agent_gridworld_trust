{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_tkinter.tkapp' object has no attribute 'is_destroy_environment'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 324\u001b[0m\n\u001b[0;32m    318\u001b[0m agents \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    319\u001b[0m     SearchAgent(num_actions, target_color\u001b[38;5;241m=\u001b[39mcolor_dict[(i \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mlen\u001b[39m(color_dict)) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m], noise_probability\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_agents)\n\u001b[0;32m    321\u001b[0m ]\n\u001b[0;32m    322\u001b[0m run(num_episodes, max_steps_per_episode, agents, num_actions, env)\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m env\u001b[38;5;241m.\u001b[39mis_destroy_environment:\n\u001b[0;32m    325\u001b[0m     env\u001b[38;5;241m.\u001b[39mdestroy_environment()\n",
      "File \u001b[1;32mC:\\Program Files\\Anaconda3\\Lib\\tkinter\\__init__.py:2410\u001b[0m, in \u001b[0;36mTk.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m   2408\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr):\n\u001b[0;32m   2409\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDelegate attribute access to the interpreter object\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 2410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtk, attr)\n",
      "\u001b[1;31mAttributeError\u001b[0m: '_tkinter.tkapp' object has no attribute 'is_destroy_environment'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Multi-Agent Dynamic Grid World Environment\n",
    "Created by: Ardianto Wibowo\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import random\n",
    "import sys\n",
    "\n",
    "# فتح ملف نصي لكتابة النتائج\n",
    "log_file = open(\"output_results.txt\", \"w\")\n",
    "\n",
    "# إعادة توجيه المخرجات المطبوعة إلى الملف النصي\n",
    "sys.stdout = log_file\n",
    "# Add the path to the 'env' folder to sys.path\n",
    "sys.path.append('env')\n",
    "\n",
    "from ma_gridworld import Env\n",
    "\n",
    "# Define color dictionary to map targets to colors\n",
    "color_dict = {\n",
    "    1: \"blue\",\n",
    "    2: \"green\",\n",
    "    3: \"orange\",\n",
    "    4: \"purple\",\n",
    "    5: \"cyan\",\n",
    "    6: \"magenta\"\n",
    "}\n",
    "\n",
    "class SearchAgent:\n",
    "    def __init__(self, num_actions, target_color, noise_probability):\n",
    "        self.num_actions = num_actions\n",
    "        self.targets_seen = []  # List of seen targets\n",
    "        self.memory = []  # Memory to track visited targets\n",
    "        self.target_color = target_color  # The color this agent is looking for\n",
    "        self.noise_probability = noise_probability  # Probability of noise in data\n",
    "    \n",
    "    # الدالة الجديدة\n",
    "    def get_target_color(self, target_coordinate):\n",
    "        \"\"\"\n",
    "        Get the color of the target based on its data in memory or predefined rules.\n",
    "        \"\"\"\n",
    "        for record in self.memory:\n",
    "            if record[\"location\"] == target_coordinate:\n",
    "                target_data = record[\"data\"]\n",
    "                target_id = int(target_data.split('_')[1])  # Extract the target ID\n",
    "                return color_dict.get(target_id, None)  # Return the corresponding color\n",
    "        return None  # Return None if the target is not in memory\n",
    "    \n",
    "    # باقي الدوال كما هي\n",
    "\n",
    "\n",
    "  \n",
    "    def analyse_sensor_data(self, agent_id, coordinate_observation, sensor_data_observation):\n",
    "        for i in range(len(sensor_data_observation)):\n",
    "            for j in range(len(sensor_data_observation[i])):\n",
    "                data = sensor_data_observation[i][j]\n",
    "                location = [\n",
    "                    coordinate_observation[0] + j - len(sensor_data_observation[i]) // 2,\n",
    "                    coordinate_observation[1] + i - len(sensor_data_observation) // 2\n",
    "                ]\n",
    "                if data is not None and 'target_' in data:  # التحقق من وجود هدف\n",
    "                    self.update_memory(location, data, agent_id)  # استدعاء update_memory\n",
    "\n",
    "                # تطبيق الضوضاء إذا كانت مفعلة\n",
    "                if is_noise_enabled and random.random() < self.noise_probability:\n",
    "                    noisy_location = [\n",
    "                        location[0] + np.random.choice([-1, 0, 1]),\n",
    "                        location[1] + np.random.choice([-1, 0, 1])\n",
    "                    ]\n",
    "                    print(f\"Agent {agent_id}: Noise applied. Original: {location}, Noisy: {noisy_location}\")\n",
    "                    location = noisy_location\n",
    "\n",
    "                # إزالة الأهداف التي تم جمعها\n",
    "                if location[0] == coordinate_observation[0] and location[1] == coordinate_observation[1]:\n",
    "                    if location in self.targets_seen:\n",
    "                        self.targets_seen.remove(location)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    def analyse_communication(self, comm_observation, agent_id):\n",
    "        \"\"\"\n",
    "        Analyze communication data and apply noise if enabled.\n",
    "        \"\"\"\n",
    "        for comm in comm_observation:\n",
    "            origin_location = comm[0]\n",
    "            sensor_data_observation = comm[1]\n",
    "            reported_by = comm[2]\n",
    "\n",
    "            print(f\"Agent {agent_id}: Received communication from Agent {reported_by} at {origin_location}\")\n",
    "\n",
    "            for i in range(len(sensor_data_observation)):\n",
    "                for j in range(len(sensor_data_observation[i])):\n",
    "                    data = sensor_data_observation[i][j]\n",
    "                    location = [\n",
    "                        origin_location[0] + j - len(sensor_data_observation[i]) // 2,\n",
    "                        origin_location[1] + i - len(sensor_data_observation) // 2\n",
    "                    ]\n",
    "\n",
    "                    # Apply noise based on probability\n",
    "                    if is_noise_enabled and random.random() < self.noise_probability:\n",
    "                        noisy_location = [\n",
    "                            location[0] + random.choice([-1, 0, 1]),\n",
    "                            location[1] + random.choice([-1, 0, 1])\n",
    "                        ]\n",
    "                        print(f\"Agent {agent_id}: Noise applied. Original: {location}, Noisy: {noisy_location}\")\n",
    "                        location = noisy_location\n",
    "\n",
    "                    # Extract color and check if it matches the target\n",
    "                    target_color = color_dict.get(int(data.split('_')[1]), None) if data and '_' in data else None\n",
    "                    if target_color == self.target_color:\n",
    "                        if location not in self.targets_seen:\n",
    "                            self.targets_seen.append(location)\n",
    "                            print(f\"Agent {agent_id}: Added target at {location} with color {target_color} from Agent {reported_by}\")\n",
    "                  \n",
    "\n",
    "                           \n",
    "                            \n",
    "    def update_memory(self, location, data, agent_id):\n",
    "\n",
    "        print(f\"Agent {agent_id} is updating memory for location {location} with data {data}\")\n",
    "        if location not in [record[\"location\"] for record in self.memory]:\n",
    "            # add\n",
    "            self.memory.append({\"location\": location, \"data\": data})\n",
    "            print(f\"Agent {agent_id} Updated Memory: {location} -> {data}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def select_action(self, coordinate_observation, agent_id):\n",
    "        print(f\"Agent {agent_id}: Current targets seen: {self.targets_seen}\")\n",
    "        if len(self.targets_seen) > 0:\n",
    "            closest_target = None\n",
    "            closest_target_distance = 999999\n",
    "            for target_coordinate in self.targets_seen:\n",
    "                # حساب المسافة المانهاتنية\n",
    "                horizontal_distance = target_coordinate[0] - coordinate_observation[0]\n",
    "                vertical_distance = target_coordinate[1] - coordinate_observation[1]\n",
    "                distance = abs(horizontal_distance) + abs(vertical_distance)\n",
    "    \n",
    "                # إضافة شروط إضافية هنا\n",
    "                target_color = self.get_target_color(target_coordinate)  # احصل على لون الهدف\n",
    "                if target_color != self.target_color:\n",
    "                    print(f\"Agent {agent_id}: Skipping target at {target_coordinate} (Color mismatch)\")\n",
    "                    continue  # تخطي الهدف إذا كان اللون لا يطابق\n",
    "                \n",
    "                # إضافة عقوبات/أوزان\n",
    "                if target_coordinate[0] < 2 or target_coordinate[1] < 2:  # إذا كان قريبًا من الحدود\n",
    "                    distance += 5  # إضافة عقوبة\n",
    "    \n",
    "                # التحقق من أقرب هدف\n",
    "                if distance < closest_target_distance:\n",
    "                    closest_target_distance = distance\n",
    "                    closest_target = target_coordinate\n",
    "    \n",
    "            if closest_target:\n",
    "                print(f\"Agent {agent_id}: Moving towards target at {closest_target}\")\n",
    "                horizontal_distance = closest_target[0] - coordinate_observation[0]\n",
    "                vertical_distance = closest_target[1] - coordinate_observation[1]\n",
    "                if abs(horizontal_distance) >= abs(vertical_distance):\n",
    "                    return 3 if horizontal_distance < 0 else 4\n",
    "                else:\n",
    "                    return 1 if vertical_distance < 0 else 2\n",
    "        else:\n",
    "            print(f\"Agent {agent_id}: No targets seen, taking random action.\")\n",
    "            return np.random.choice(self.num_actions - 1) + 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_action(agent_id, observation, num_actions, agents, env):\n",
    "    \"\"\"\n",
    "    This method provides a random action chosen recognized by the ma-gridworld environment:\n",
    "    1: up, 2: down, 3: left, 4: right, 0: stay\n",
    "    \"\"\"\n",
    "    \n",
    "    coordinate_observation = tuple(observation[0])  # Keep observation as (x, y) tuple\n",
    "\n",
    "    # Optional observation data may be used, depend on the agent needs.\n",
    "    win_state_observation = observation[1]\n",
    "    sensor_data_observation = observation[2]\n",
    "    comm_observation = observation[3]\n",
    "    \n",
    "\n",
    "    print(f\"Observation for Agent {agent_id}: {observation}\")\n",
    "\n",
    "    # Check if communication data is available\n",
    "    if comm_observation:\n",
    "        agents[agent_id].analyse_communication(comm_observation, agent_id)\n",
    "    else:\n",
    "        print(f\"Agent {agent_id}: No communication data available this step.\")\n",
    "\n",
    "    # Analyze sensor data\n",
    "    agents[agent_id].analyse_sensor_data(agent_id, coordinate_observation, sensor_data_observation)\n",
    "    \n",
    "    # Select physical action\n",
    "    physical_action = agents[agent_id].select_action(coordinate_observation, agent_id)\n",
    "\n",
    "    # Define communication action\n",
    "    if env.is_agent_silent:\n",
    "        comm_action = []  # Communication action is set to be zero if agent silent\n",
    "    else:\n",
    "        comm_action = [coordinate_observation, sensor_data_observation, agent_id] \n",
    "    \n",
    "    return (physical_action, comm_action)\n",
    "\n",
    "\n",
    "def run(num_episodes, max_steps_per_episode, agents, num_actions, env):\n",
    "    for episode in range(num_episodes):\n",
    "        print(f\"Starting episode {episode + 1}\")\n",
    "        observations = env.reset()  # Reset the environment at the start of each episode\n",
    "        \n",
    "        # Reset targets_seen and memory for each agent\n",
    "        for agent in agents:\n",
    "            agent.targets_seen = []  # Reset the seen targets\n",
    "            agent.memory = []  # Reset the memory\n",
    "        \n",
    "        done = [False] * env.num_agents  # Initialize 'done' as a list for each agent\n",
    "        step_count = 0\n",
    "\n",
    "        while not all(done) and step_count < max_steps_per_episode:  # Stop if all agents are done or max steps reached\n",
    "            actions = []\n",
    "            next_observations = []\n",
    "            \n",
    "            for agent_id in range(env.num_agents):\n",
    "                observation = observations[agent_id]\n",
    "                action = get_action(agent_id, observation, num_actions, agents, env)\n",
    "                \n",
    "                actions.append(action)\n",
    "                next_observations.append(observation)\n",
    "\n",
    "            next_observations, rewards, done = env.step(actions)  # Step in the environment\n",
    "\n",
    "            observations = next_observations\n",
    "            step_count += 1\n",
    "\n",
    "            # Render the environment\n",
    "            env.render()\n",
    "\n",
    "            # طباعة النتائج على الشاشة\n",
    "            print(f\"Step {step_count}:\")\n",
    "            for agent_id in range(env.num_agents):\n",
    "                print(\n",
    "                    f\"  Agent {agent_id}: Observation: {observations[agent_id]}, \"\n",
    "                    f\"Action: {actions[agent_id]}, Reward: {rewards[agent_id]}, Done: {done[agent_id]}\"\n",
    "                )\n",
    "\n",
    "        # طباعة عدد الخطوات عند نهاية الحلقة\n",
    "        print(f\"Episode {episode + 1} finished after {step_count} steps.\\n\")\n",
    "        \n",
    "        sys.stdout = sys.__stdout__\n",
    "        log_file.close()\n",
    "\n",
    "        print(\"تم تخزين جميع النتائج في ملف output_results.txt.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    gsize=15 #grid size (square)\n",
    "    gpixels=30 #grid cell size in pixels\n",
    "\n",
    "    is_sensor_active = True #True:  Activate the sensory observation data\n",
    "    sensory_size = 3 #'is_sensor_active' must be True. The value must be odd, if event will be converted to one level odd number above\n",
    "    \n",
    "    num_agents = 10 #the number of agents will be run in paralel\n",
    "    num_obstacles = 0 #the number of obstacles\n",
    "    is_single_target = False #True: all agents have a single target, False: each agent has their own target\n",
    "    num_targets_per_agent = 5 #'is_single_target' must be true to have an effect\n",
    "    \n",
    "    is_agent_silent = False #True: communication among agents is allowed\n",
    "    \n",
    "    \n",
    "    is_noise_enabled = True  # تفعيل الضوضاء\n",
    "    \n",
    "   \n",
    "\n",
    "    num_episodes=1 #the number of episode will be run\n",
    "    max_steps_per_episode=1000 #each episode will be stopped when max_step is reached\n",
    "\n",
    "    eps_moving_targets = 10 #set this value greater than 'num_episodes' to keep the targets in a stationary position\n",
    "    eps_moving_obstacles = 10 #set this value greater than 'num_episodes' to keep the obstacles in a stationary position\n",
    "\n",
    "    render = True #True: render the animation into the screen (so far, it is still can not be deactivated)\n",
    "\n",
    "    min_obstacle_distance_from_target = 1 #min grid distance of each obstacles relative to targets\n",
    "    max_obstacle_distance_from_target = 5 #max grid distance of each obstacles relative to targets\n",
    "    min_obstacle_distance_from_agents = 1 #min grid distance of each obstacles relative to agents\n",
    "\n",
    "    reward_normal = -1 #reward value of normal steps\n",
    "    reward_obstacle = -5 #reward value when hit an obstacle\n",
    "    reward_target = 50 #reward value when reach the target\n",
    "\n",
    "    is_totally_random = True #True: target and obstacles initial as well as movement position is always random on each call, False: only random at the beginning. \n",
    "    animation_speed = 0.1 #smaller is faster \n",
    "    is_destroy_environment = True #True: automatically close the animation after all episodes end.  \n",
    "\n",
    "    # Initialize environment\n",
    "    env = Env(\n",
    "        num_agents=num_agents, num_targets_per_agent=num_targets_per_agent, num_obstacles=num_obstacles,\n",
    "        eps_moving_obstacles=eps_moving_obstacles, eps_moving_targets=eps_moving_targets,\n",
    "        is_agent_silent=is_agent_silent, is_single_target=is_single_target, sensory_size=sensory_size,\n",
    "        gpixels=gpixels, gheight=gsize, gwidth=gsize, is_sensor_active=is_sensor_active,\n",
    "        min_obstacle_distance_from_target=min_obstacle_distance_from_target,\n",
    "        max_obstacle_distance_from_target=max_obstacle_distance_from_target,\n",
    "        min_obstacle_distance_from_agents=min_obstacle_distance_from_agents,\n",
    "        is_totally_random=is_totally_random, animation_speed=animation_speed,\n",
    "        reward_normal=reward_normal, reward_obstacle=reward_obstacle, reward_target=reward_target\n",
    "    )\n",
    "    \n",
    "    num_actions = len(env.action_space)\n",
    "    agents = [\n",
    "        SearchAgent(num_actions, target_color=color_dict[(i % len(color_dict)) + 1], noise_probability=0.2)\n",
    "        for i in range(num_agents)\n",
    "    ]\n",
    "    run(num_episodes, max_steps_per_episode, agents, num_actions, env)\n",
    "\n",
    "    if env.is_destroy_environment:\n",
    "        env.destroy_environment()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
